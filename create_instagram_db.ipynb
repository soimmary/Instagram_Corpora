{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project: Instagram Corpora üíÅüèº‚Äç‚ôÄÔ∏èüíÖüèª‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_sqlalchemy import SQLAlchemy\n",
    "from flask import Flask, render_template, request\n",
    "import sqlite3\n",
    "\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from razdel import sentenize, tokenize\n",
    "from instagrapi import Client\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "pymorph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collecting data from Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(token):\n",
    "    p = pymorph.parse(token)[0]\n",
    "    return p.normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return [i for i in nltk.word_tokenize(text) if i.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_instagram(user_ids_list, file_path, USERNAME='your username', \n",
    "                             PASSWORD='your password', number_of_posts=50):\n",
    "    insta_cl = Client()\n",
    "    insta_cl.login(USERNAME, PASSWORD)\n",
    "    \n",
    "    file = open(file_path, 'a', encoding='utf-8')\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    sent_id = 1\n",
    "    for account in tqdm(user_ids_list):\n",
    "        user_id = insta_cl.user_id_from_username(account)\n",
    "        medias = insta_cl.user_medias(user_id, number_of_posts)\n",
    "        for post in medias:\n",
    "            sentences = [substring.text.replace('\\n', ' ') for substring in sentenize(post.caption_text)]\n",
    "            for sent in sentences:\n",
    "                if sent:\n",
    "                    writer.writerow([sent_id, sent, 'https://www.instagram.com/p/' + post.code])\n",
    "                    sent_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of insta accounts\n",
    "user_ids_list = [link.strip() for link in open('instagram_ids.txt', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [04:05<00:00, 24.59s/it]\n"
     ]
    }
   ],
   "source": [
    "get_texts_from_instagram(user_ids_list, 'instagram_texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DB desigh:** \n",
    "1. A table with context information: context id, context, metadata.\n",
    "2. A table with morphological features of tokens: id, context id, token, lemma, pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('insta_corpus.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS context_metadata \n",
    "(id_cm int PRIMARY KEY, \n",
    "context text,\n",
    "metadata text)\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS morphology \n",
    "(id_morph int PRIMARY KEY,\n",
    "context_id int,\n",
    "token text,\n",
    "lemma text,\n",
    "pos text\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing texts and collecting morphological information of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('instagram_texts.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    sentences = [line for line in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '–ù–æ –¥–∞–≤–∞–π—Ç–µ —á–µ—Å—Ç–Ω–æ.', 'https://www.instagram.com/p/CU7q4SBjT8q']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the 1st table with context id, context and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('insta_corpus.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "for context_id, context, metadata in sentences:\n",
    "    cur.execute(\"INSERT or IGNORE INTO context_metadata VALUES (?,?,?)\",\n",
    "                [context_id, context, metadata]) \n",
    "    \n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_metadata_for_unigram(unigram_info):\n",
    "    matches_list = []\n",
    "    item = unigram_info[0]\n",
    "    tag = unigram_info[1]\n",
    "    \n",
    "    conn = sqlite3.connect('insta_corpus.db')\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    for elem in unigram_info:\n",
    "        id_num = elem['_id']\n",
    "        context = elem['context']\n",
    "        metadata = elem['metadata']\n",
    "        matches_list.append((context, metadata))\n",
    "    return matches_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the 2nd table with token id, token, lemma, pos, context and metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function which outputs a dictionary with token id, token, lemma, pos, context and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pymorphy_token_analysis(token, id_num, context_id, context, metadata):\n",
    "    p = pymorph.parse(token)[0]\n",
    "    token_info = defaultdict()\n",
    "    token_info['id'] = id_num\n",
    "    token_info['token'] = token\n",
    "        \n",
    "    if p.tag.POS == 'INFN':\n",
    "        token_info['POS'] = 'verb'\n",
    "        token_info['inf'] = 'inf'\n",
    "\n",
    "    elif p.tag.POS in ['PRTF', 'PRTS']:\n",
    "        token_info['POS'] = 'verb'\n",
    "        token_info['partcp'] = 'partcp'\n",
    "            \n",
    "    elif p.tag.POS in ['ADJF', 'ADJS']:\n",
    "        token_info['POS'] = 'adj'\n",
    "        \n",
    "    elif p.tag.POS == 'GRND':\n",
    "        token_info['POS'] = 'verb'\n",
    "        token_info['ger'] = 'ger'\n",
    "    else:\n",
    "        try:\n",
    "            token_info['POS'] = p.tag.POS.lower()\n",
    "        except AttributeError:\n",
    "            token_info['POS'] = None\n",
    "    \n",
    "    token_info['lemma'] = pymorph.parse(token)[0].normal_form\n",
    "    token_info['context_id'] = context_id\n",
    "    token_info['context'] = context\n",
    "    token_info['metadata'] = metadata\n",
    "        \n",
    "    return token_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('insta_corpus.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "id_num = 1\n",
    "for sent in sentences:\n",
    "    context_id = sent[0]\n",
    "    context = sent[1]\n",
    "    metadata = sent[2]\n",
    "    for token in tokenizer(context):\n",
    "        token_dict = pymorphy_token_analysis(token, id_num, context_id, context, metadata)\n",
    "        cur.execute(\"INSERT or IGNORE INTO morphology VALUES (?,?,?,?,?)\",\n",
    "                [token_dict['id'], token_dict['context_id'], token_dict['token'], \n",
    "                 token_dict['lemma'], token_dict['POS']])\n",
    "        id_num += 1\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
